# NLP проект: определение "токсичных" комментариев
Цель проекта — создать модель, которая сможет выявить "токсичные" комментарии, которые следует отправить на модерацию.
### Стек проекта:
`python`, `numpy`, `pyplot`, `sklearn`, `statsmodels`, `catboost`, `nltk`

### Результат проекта:
В ходе работы нами были загружены и изучены текстовые данные с комментариями англоязычных пользователей. При изучении целевого признака было замечено, что в датасете имеется дисбаланс классов — по этой причине в дальнейшей работе у всех моделей использовался параметр class_weight.

Перед обучением модели тексты были лемматизированы т.е. приведены к начальной форме. Затем мы применили объект TfidfVectorizer для подсчета TF-IDF по корпусу текстов (комментариев) и таким образом создали векторизированные признаки.

После создания признаков, весь датасет был разделен на 3 выборки в соотношении 3:1:1 — обучающую, валидационную и тестовую. Было обучено три модели — LogisticRegression, DecisionTreeClassifier, CatBoostClassifier. Их работа была проверена на валидационной выборке — лучший результат показала модель LogisticRegression. Она же стала единственной моделью, которая смогла добиться необходимого значения целевой метрики F1 — более 0.75.

С целью оценить адекватность работы лучшей модели и возможность ее дальнейшего использования на незнакомых текстах, мы оценили ее работу на тестовой выборке.

Логистическая регрессия пригодна для работы с незнакомыми текстами, и более того, быстро обучается и предсказывает. Для повышенной точности можно применить более долго обучающиеся модели или подготовить текст и признаки с помощью внешних библиотек (например, BERT).
